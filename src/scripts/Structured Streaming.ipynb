{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["###Structure Streaming of Yelp Dataset\n\nStructured Streaming is a stream processing engine that lets you express computation on streaming data in the same way you express a batch computation on static data. The Spark SQL engine performs the computation incrementally and continuously updates the result as streaming data arrives"],"metadata":{}},{"cell_type":"code","source":["##Importing yelp dataset\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\ninputPath = \"dbfs:/mnt/yelp-mount\"\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)"],"metadata":{}},{"cell_type":"code","source":["jsonSchema = StructType([ \n  StructField(\"review_id\", StringType(), True), \n  StructField(\"user_id\", StringType(), True) , \n  StructField(\"business_id\", StringType(), True) , \n  StructField(\"stars\", IntegerType(), True) , \n  StructField(\"date\", TimestampType(), True) , \n  StructField(\"text\", StringType(), True), \n  StructField(\"useful\", IntegerType(), True), \n  StructField(\"funny\", IntegerType(), True), \n  StructField(\"cool\", IntegerType(), True) ])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Stream DataFrame representing data in the JSON files\nstreamInputDF = (\n  spark\n    .readStream\n    .schema(jsonSchema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .json(inputPath)\n)\n\n# Query for counting review grouped by stars\nstreamingCountsDF = (                 \n  streamInputDF\n    .groupBy(\n      streamInputDF.stars, \n      window(streamInputDF.date, \"1 hour\"))\n    .count()\n)\n\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["As we can see, streamingCountsDF is a streaming Dataframe (streamingCountsDF.isStreaming was true). We can start streaming computation, by defining the sink and starting it. In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be a in a in-memory table (note that this for testing purpose only in Spark 2.0)."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Let's wait a bit for a few files to be processed and then query the in-memory counts table."],"metadata":{}},{"cell_type":"code","source":["from time import sleep\nsleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql select stars, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, stars"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["sleep(5)  # wait a bit more for more data to be computed"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Also, let's see the total number of each rating"],"metadata":{}},{"cell_type":"code","source":["%sql select stars, sum(count) as total_count from counts group by stars order by stars"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"Assignment3","notebookId":1236743218157963},"nbformat":4,"nbformat_minor":0}
